
#面试经验# 京东/拼多多/BIGO/陌陌/苏宁/浦发
方向：ML/DL/CV/NLP
- 决策树怎么解决回归问题?
- XGBoost 过拟合了，怎么调参数？
- 直方图均衡化知道吗？说下算法思路？
- asoftmax和arcface的损失是怎样的？
- lstm和RNN对于哪些问题题更擅长？
- 从2w条样本中采样2000条点赞数最高的怎么做？
- pytorch中多卡训练的过程是怎样的？说下gather scatter是怎么做的？
- 三个碗只有一个有球，你选定一个后我拿走一个没有球的碗，你要不要更换你选定的碗？
- 代码题：
（1）n个骰子的同时放下，上面朝上的点数总和为s的概率是多少？
（2）给一个数组和滑动窗口大小，求滑动窗口内的最大值是多少？


很幸运地遇到了和我做的方向非常match的组，boss也很有魅力，极速面试，菜鸡能够过面试很幸运很幸运，不出意外就上岸了吧，给大家分享一下我的面经，可能有些不全，回想起多少写多少


【商汤1面】



1. CornerNet介绍，CornerPooling是怎么做的，怎么解决cornernet检测物体合并为一个框的问题


2. 介绍Mimic知识蒸馏是怎么做的



3. MobileNet 介绍



4. 普通卷积、DW PW卷积计算量推导



5. MobileNet V2中的Residual结构最先是哪个网络提出来的


编程：


1. 之子形打印二叉树


2. MxN的方格中有多少个正方形、多少个矩形、有多少种不同面积矩形

【商汤2面】


1. 在人脸关键点和检测中的mimic是怎么做的？为什么不在logits输出上做？用l2 loss吗？



2. 人脸关键点使用pose做multitask为什么landmark会有提升？



3. 目标检测在工程中应用有没有遇到一些问题？检测类别冲突怎么办？


4. 对机器学习了解多吗？


5. 现有两个特征向量，怎么分析他们的相似度？


6. 有没有什么数学方法能够去除特征矩阵中的噪声？

编程：


1. 判断二叉树是否包含另一二叉树


2. 有序数组合并

【商汤3面-leader面】


主要与大boss聊自己的未来规划、对某个大方向做一些自己的分析


1. 介绍你在xx项目中的工作


2. 项目中你用做过SDK和安卓开发，是在这个项目中学的吗？


3. 你认为目前video和知识蒸馏这两个方向的挑战和可以改进的地方在哪


4. RNN为什么long-term dependency做不好


5. 你用了Memory Network，有提升吗



6. 你觉得网络模型和硬件平台是什么关系



7. 未来有什么打算，我带的两个组你想去哪个


8. 更想做出一个实用的产品还是做研究

【HR面】


主要介绍了公司的一些情况、福利，询问手中的offer情况，聊天

_______________

2019.08.20 更新：

已收到录用通知书，已提前入职商汤实习



#面试经验# 商汤科技/旷视科技/拼多多/360/海康威视/BIGO
方向：ML/DL/CV

- 普通卷积、DW PW卷积计算量推导
- CornerNet介绍，CornerPooling是怎么做的
- MobileNet V2中的Residual结构最先是哪个网络提出来的
- GBDT中的CART回归树每个叶子节点的输出值是什么
- L1，L2的区别，L2为什么可以减轻过拟合
- 两个卷积核大小为（3,3），步长为（2，2）的卷积层堆叠，上层感受野大小是？
- 算法题，如何判断一个点在一个凸多边形里。
- 算法题，求两个链表的第一个公共节点
- 算法题：random_shuffle的实现
- 数学题，已知a,b都服从均值为0，方差为1的正态分布，求max(a,b)的期望
- 编程题：有序数组合并
- 编程题：2个有序链表的合并到多个有序链表的合并

#面试经验# 百度/网易/小米/商汤/作业帮/寒武纪/科大讯飞
方向：ML/DL/NLP/CV

- LR怎么加入非线性？
- yolov3中的anchor怎么生成的
- 梯度消失与梯度爆炸原因？解决方案
- Transformer/GPT/BERT 的原理讲解
- CenterNet具体是如何工作的，介绍一下其损失函数
- BN层怎么具体实现的(问的很具体，举例子说明)
- 各种激活函数介绍一下，优缺点及适用场景
- 怎么判断一个字符串是否是日期
- 过拟合、欠拟合怎么判断，解决
- SVM为什么叫支持向量机、损失函数是什么、核函数
- xgboost，每个叶子节点的值是怎么确定的？
- 编程题：青蛙跳台阶、变态青蛙跳台阶

#面试经验# 百度/字节跳动/小米/海康威视/猿辅导/BIGO
方向：ML/DL/CVNLP

- 解释一下菱形继承
- LSTM减弱梯度消失的原理
- 设计一个在CNN卷积核上做dropout的方式
- map的底层是用什么实现的，查询时间复杂度，插入和删除呢
- L1为什么能稀疏矩阵  L2为什么不能，L2为什么能解决过拟合
- 算法题：二叉树中的最长路径
- 算法题：x的平方根
- 算法题：二分查找
- 算法题：最大堆的插入
- 算法题：带括号的加减乘除字符串运算
- 算法题：最长连续递增序列
- 算法题：最长不连续序列
- 算法题：无序数组构建一棵二叉排序树


1面(1h10min) - 电面



约定电面晚上8点半(阿里是加班到9、10点的节奏？）



自我介绍：



项目



主要是商汤无人车实习的项目，问我比baseline提升15个点，怎么来的。



从数据迭代、backbone、模型修改几个层面上说了下。



挑一两个有意思的优化说说，说了cascade、hdcnn的结构，为什么用这种结构。



项目中出现什么情况，怎么解决的？主要就是说小目标检测的解决方案。



对caffe源码熟悉程度。（我扯了扯源码的底层设计模式，数据流怎么流的，如何添加新层、cuda代码的细节）



开放题



给了一个情景，如何训练模型、调优。（题目很空，主要考察你对深度学习的理解）



根据需求（前向传播时间、模型大小），确定模型和基础网络，跑第一版模型。（举了个栗子）

判断模型是否出现过拟合的情况，来决定下一步的优化方向。

结果分析(confusionMatrix等)，分析问题，将论文中的方法套上去，如果没有自己创造。（又举了个栗子）



softmax、多个logistic的各自的优势？1、类别数爆炸，2、推了下softmax反向传播的公式，来对比两者的优劣。



算法(走流程题)

字符串判断是否是ipv4，c++。(可能是时间不多了，大佬想下班了)



体会：



全程大多都是我在说，没有太多互动。后来经过源神@邢源建议，还是要故意给面试官漏点马脚让他们来怼我们，然后再怼回去，并说明不这么做的原因，不然不好拿高评分。(卧槽，真的是套路深啊～)



2面(1h30min) - 杭州电面



项目



大佬貌似涉猎很广泛，对每一个领域都很熟悉，基本上简历中的很多细节，他都能找到点怼我。（聊了很久）



项目是从头怼到尾，主要考察对项目、深度学习的理解。



大佬对我的trickList很感兴趣，我猜想他现在做的工作和我的很相似。



Anchor大小、长宽比选取？我说了业界常用的方法(YOLO9000中的方法)，并提了一个更优的方法。



如何解决小目标：



为什么要深层、浅层featureMap concat？提了点细节和我踩的坑，需要数量级上的调整，不然深层的feature可能会被压制。



Cascade的思想? 说了下我的摸索的一个过程。改变样本分布，困难样本挖掘，能达到比较好的效果。



文字识别使用ctc loss的一些细节。



开放题



设计一个情景，倾斜字体检测，问我有什么好的想法？（我觉得应该是他现在遇到的问题）



数据增强，加入形变扰动。



非end-to-end版本：分别训练检测和分类，举了之前做过的一个文字识别的项目的实现。



end-to-end版本：加入仿射变换学习因子，学习字体倾斜的角度和形变。



在商汤发论文了吗？



没有，正在攒，项目比较重，但有一些work和insight，讲了下思路。（大佬听的很认真，貌似被我的故事打动了[捂脸]）



为啥要换实习？日常吹水。



评价：大佬主动评价我对模型理解挺好的，工作做的挺深的，说等下一面吧。



体会：二面面试官说话很快，思维比较敏捷，觉得和这种人讨论问题很欢畅，如果一起工作会很赞。



以后面试说话语速应该快一些，让人觉得思维比较敏捷，这个可能会有加分项吧。



3面(1h) - 电面



项目&基础



大佬应该是搞backbone模型优化的，问了我怎么迭代基础网络的版本的，日常扯论文，自己的实验结果和理解。



前两个卷积层通道数不用很多，主要是提取边缘、颜色信息，少量的卷积核足矣。



skip connection有什么好处？推了下反向传播公式，根据链式法则，梯度可以直接作用于浅层网络。



初始学习率怎么设？这个我真的没有总结过，只是说一般使用0.01～0.1。



mobileNet、shufflenet的原理？说了下原理。



为什么mobileNet在理论上速度很快，工程上并没有特别大的提升？先说了卷积源码上的实现，两个超大矩阵相乘，可能是group操作，是一些零散的卷积操作，速度会慢。



大佬觉得不满意，说应该从内存上去考虑。申请空间？确实不太清楚。



问我看过哪些前沿的论文？说了说最近两个月的优质的论文。



扯到了tripleLoss，大佬问样本怎么选择？随机，然后就被大佬嫌弃了。装逼失败，这块确实没怎么深入研究。



为什么用multiLoss？多loss权重如何选？训练普通的模型使其收敛，打印反向传播梯度的大小，这表示该task的难度，以此作为loss的权重，然后我补充说了下可以搞一个动态的loss权重，根据一段时间窗口来决定loss的权重。



开放性问题



凸优化了解吗？牛顿法、SGD、最小二乘法，各自的优势。



凸优化其他东西呢？我说只有一些零散的知识点的记忆，纯数学，没有很系统的研究。(面试官貌似数学功底很好，只能认怂)。



感觉有点虚，我尝试着往我会的地方引[捂脸]。

工程上如何对卷积操作进行优化？答：傅立叶模拟卷积。大佬不满意，说那是cudnn早就实现的，还有什么优化吗？（确实不知道，甩锅给工程组）



样本不均衡怎么处理？一个batch类别均等采样，修改loss对不同样本的权重。



体会：



三面面试官懂得不少，不过最后还是过了，有时间凸优化还是要系统整理下。



4面(50min) - 交叉面



项目



大佬应该不是做深度学习的，应该是机器学习那块的。交流中能感觉出来对这块不是很熟。挑他不会的玩命说，至少让他看到我的工作量。



基础



SVM的KTT条件？说了说，说到SMO实在说不下去了。



GBDT和randomForest区别？原理角度，方差、偏差角度，过拟合角度，谈了谈之前打阿里天池的一些经验吧。



GBDT和xgboost区别？算法上工程上的优化，面试前专门看了，总结的不错，知乎，更多细节可以看看陈天奇的论文，我没看过[捂脸]，做机器学习的小伙伴最好看看。



算法题



求和接近于target的连续子数组。（lintcode上有类似的题）



最后说让后面应该还有个hr面。