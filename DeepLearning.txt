深度学习相关：
1、-- 图片检测、识别、分割的区别 
2、-- CNN
    （1）BP 手推反向传播，cnn怎么更新w值和反向传播
    （2）CNN反向传播细节，怎么过全联接层、池化层、卷积层
    （3）CNN 如何保持平移方向不变性
	
3、-- 卷积相关
   （1）1 * 1卷积核的作用，卷积输出计算，卷积核参数数量计算
   （2）3×3 卷积核 与 5×5 卷积核相比的优点
   （3）普通卷积、DW PW卷积计算量推导，计算flops，
   （4）两个卷积核大小为（3,3），步长为（2，2）的卷积层堆叠，上层感受野大小是？
   （5）感受野的计算，CNN 的感受野受什么影响
   （6）卷积是降采样过程，怎么从一个小图片回到大图片，尽量不丢失数据（下采样和上采样）
   （7）实现卷积操作（代码）

   
4、--（1）Max Pooling和 Average Pooling的区别，使用场景分别是什么？
     （2） CNN Maxpooling 怎么反向传播？
   
5、-- （1）ROC曲线和AUC曲线， 手写AUC的计算， auc比F1好在哪
      （2）召回和准确度评价怎么算，这俩是矛盾的怎么选最优
	  （3）分类、检测、分割评价指标说一下？

6、--（1）解释 L0、L1、L2正则化
     （2）L1，L2的区别，L1为什么能使特征稀疏(稀疏矩阵)  L2为什么不能，L2为什么能解决（或者减轻）过拟合

7、-- （1）Softmax计算公式，什么情况下要使用它，梯度公式推导，反向传播 和代码实现 ，softmax 的求导，反向传播
      （2）介绍分层 softmax，还有负采样？怎么负采样？
	  （3）为什么softmax是指数形式
	  
8、-- 损失函数（Loss）
      （1）常用的 Loss 函数， cross entropy的原理是什么？反向传播机制是什么？
	  （2）Loss不收敛的原因和解决方法
	  （3）损失函数正则项的本质是什么?

9、-- 批归一化Batch Normalization原理
      （1）BN 层的原理，优化过程，优化的是什么
	  （2）BN层怎么具体实现的，最好是能写出公式
      （3）BN 训练，测试区别，bn如何在inference是加速
	  
10、--各种激活函数的优缺点
      （1）各种激活函数介绍一下，优缺点及适用场景
	  （2）比如sigmod的问题在哪里，relu是怎么解决的，relu的问题在哪里，有没有对应的解决算法（prelu），介绍 Leaky Relu 并写公式 leakyrelu解决了梯度消失问题吗
	  （3）深度学习用relu激励函数，为什么，好处是什么
      （4）relu激励函数训练数据时会让神经元失活，训练之后进行应用时有什么注意的吗，训练时失活的神经元在应用时怎么办
	 
11、-- 梯度消失和梯度爆炸：
      （1）梯度消失与梯度爆炸原因？解决方案
	  （2）从数学层面分析一下
      （3）梯度下降现在为什么用的少
  
12、- 过拟合、欠拟合（overfitting,underfitting）的原因及解决方法

13、- Dropout
      （1）Dropout 的原理。为什么能防止过拟合？具体实现
      （2）Dropout 前向和反向的处理
	  （3）dropout在训练和测试时不同，怎么保证测试结果稳定
 
14、-- 怎么解决样本不均衡（重点考核损失函数优化，正负样本不均衡时的解决方案）

15、- 深度学习常用的optimizer（优化器）
     （1）SGD每步做什么，为什么能online learning
     （2）Adam优化器的迭代公式
	 
16、- 深度学习bachsize怎么训练，理由，为什么要这样训练

17、- （1）如何进行数据预处理，如何进行数据归一化
      （2）为什么要归一化？(消除数据量纲差，可以剔除一些异常值，会使得模型收敛快一些也好一些，计算友好度也会稍微好一些)
	  
18、- 数据增强{
     （1）现在有10000个数据集，怎么在现有条件下扩大训练数据集，前提在现有数据集的基础上，不去重新采集数据



目标检测

Faster-rcnn 相关
   （1）项目中使用Faster rcnn，请问Faster rcnn的优势是什么，为什么在这个项目使用Faster rcnn
   （2）Faster-rcnn RPN的作用和原理
   （3）ROI pooling 的主要作用是什么？RoI pooling和RoI align区别
   （4）Faster rcnn anchor机制，分别说一下 RPN阶段两种Loss分别是什么？faster-rcnn 损失函数，优化函数
   （5）faster-rcnn 为什么可以图片不同尺寸输入
   （6）Faster-rcnn有什么不足的地方吗？如何改进？fasterrcnn怎么优化
   （7）模型调参，fastrcnn内部怎么实现，rpn分几层，怎么调节参数
   （8）Faster-rcnn中使用的哪个基础神经网络模型，VGG还是ResNet? 
   （9）RCNN系列模型的区别
    




  -- YOLOV1~V3系列介绍，以及每一版的改进，优缺点介绍。yolov3中的anchor怎么生成的, 写出 YOLOv3 的损失函数
  -- SSD、yolo、Fast RCNN 的区别，
      RPN怎么计算 box 的实际坐标
	  为什么回归损失中用smooth L1 (faster-rcnn)
	  小目标检测有哪些trick
  -- yolo跟ssd的损失函数是什么样，有啥缺点，yolo和ssd中正样本怎么确定的 
  -- FPN结构 
  -- 目标检测中，如果检测到的目标较小时，框向内缩，该怎么解决
  -- CornerNet介绍，CornerPooling是怎么做的，怎么解决cornernet检测物体合并为一个框的问题  
  -- MobileNet 介绍，MobileNetV2中1x1卷积作用
  -- MobileNet V2中的Residual结构最先是哪个网络提出来的，MobileNetV2 module的参数量和FLOPs计算
  -- shufflenet 算法题：random_shuffle的实现
  -- CenterNet具体是如何工作的，介绍一下其损失函数
  -- 分类网络样本不均衡怎么办？
  -- Inception（V1-V4）网络结构以及优缺点
  -- Focal loss， OHEM（online Hard example mining）到底比focal loss差再哪里了
     Focal Loss是如何进行难分样本挖掘的
  -- 介绍常见的 Anchor free 目标检测算法
  -- 介绍 anchor-based和anchor-free两者的优缺点
  -- ResNet相关。描述、介绍特点、介绍为什么效果不会随着深度变差、提了一下V2的区别
     ResNet 主要研究了什么问题
  
 
扩展问题

1、 分类网络训练样本有噪声(错误标注)怎么办
2、 分类网络样本不均衡怎么办
3、 分类网络想要分很细的类(比如阿拉斯加和哈士奇)怎么办
 
开放性问题，怎么处理特征、怎么选择模型
  
分割：
  -- deeplab v3如何改进，训练过程
  -- 介绍金字塔池化，ASPP，深度可分，带孔卷积
  -- 语义分割中CRF的作用
  -- HMM 和 CRF的区别
  -- CRF 怎么训练的（传统+深度学习）


  -- 剪枝压缩，些精简网络 （tplink）
  -- 介绍Mimic知识蒸馏是怎么做的


--- code 编程
 -- 卷积底层的实现方式（如caffe里面的img2col）
 -- NMS 描述，手撕 NMS
 -- numpy实现交叉熵
 -- 编程题：编程实现目标检测中的 IoU 计算
 -- 例如计算flops，卷积维度变换的公式推导，卷积是如何编程实现的；
 -- pytorch中多卡训练的过程是怎样的？说下gather scatter是怎么做的？
 -- 设计一个在CNN卷积核上做dropout的方式
 . 编程题，写目标检测里的NMS和IoU

就项目中数据处理方式做了详细的询问，生成的多张数据集如何使用，缺失值的处理需要考察到哪些问题，均值填充是否科学等
- 
CNN是深度深好还是长宽大好
样本不均衡怎么搞（重点考核损失函数优化）
问了FCOS和SSD之间的速度等的对比

自注意力机制



时间片t的计算依赖t-1时刻的计算结果，这样限制了模型的并行能力；
顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM依旧无能为力。
如何做pytorch的部署与热更新


缺失值如何处理，什么情况下均值、众数，什么情况下丢弃特征。

- 编程题：写目标检测中的NMS和IoU
- 编程题：用 PyTorch写一下大致的train val的流程


