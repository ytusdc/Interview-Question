深度学习相关：

卷积输出计算，卷积核参数数量

1、   图片检测、识别、分割的区别 ---- 进神经网络时第一个不同是，识别可以resize但是检测对位置敏感不能resize（传统上说，现在可以resize，但是记住缩放信息，能够还原）
	-- 手推梯度反向传播
	-- 介绍熟悉的NASNet网络
	-- 常用的凸优化方法，介绍
2、CNN
	(1)  BP 算法手推反向传播，cnn怎么更新w值和反向传播， 手推CNN公式
	(2)  CNN反向传播公式推导，怎么过全联接层、池化层、卷积层
    (3)  CNN的平移不变性是什么？如何实现的？
    (4)  CNN是深度深好还是长宽大好
    (5)  DNN反向传播公式推导
	
	
3、-- 卷积相关
   	(1) 1 * 1卷积核的作用，哪些情况可以使用1x1卷积？
   	(2) 3×3 卷积核 与 5×5 卷积核相比的优点，卷积操作是线性的吗？CNN是线性的吗？为什么？
	(3) 感受野的计算，CNN 的感受野受什么影响
   	(4) 卷积输出计算，卷积核参数数量计算：普通卷积、DW PW卷积计算量推导，计算flops，网络参数量计算
   	(5) 卷积是降采样过程，怎么从一个小图片回到大图片，尽量不丢失数据(下采样和上采样) 
   	(6) 实现卷积操作(代码) 
   	(7) 卷积如何加速
   	(8) 卷积神经网络的卷积核大小、个数,卷积层数如何确定呢?
   	(9) 卷积层如何修剪，量化等
   	(10) 上采样方法，反卷积
   	(11) 空洞卷积如何实现
	(12) deformable conv怎么做,具体怎么学的，对偏移有没有什么限制
 
	
4、--池化的作用和种类
	(1) Max Pooling和 Average Pooling的区别，使用场景分别是什么？
	(2) 哪些情况用 MaxPool比AveragePool效果好？原因
	(3) pooling反向传播
	(4) RoI Pooling和RoI Align区别
	(5) 当出现一个大噪点时，用哪种池化更好
	(6) 除了池化层还有什么方法可以减少特征数量(这里他提到了UNet模型，之前有看过，就说出来了，是采样的方法) 

5、-- 
	(1) 精确率，召回率，和准确度评价怎么算，这俩是矛盾的怎么选最优
	(2) ROC曲线和AUC曲线，介绍F1-score, auc比F1好在哪
	(3) auc比F1好在哪-- ROC曲线 vs Precision-Recall曲线，各自的使用场景选择
	(4) Log Loss 和 AUC 的区别， 适用于什么场景
	(5) 分类、检测、分割评价指标说一下？
	(6) 手写AUC曲面面积的计算(或者伪代码)
	

6、--
	(1) L0、L1、L2定义，L1，L2正则化原理，从参数分布和参数图上解释
	(2) L1，L2 的区别，L1为什么能使特征稀疏，L2为什么不能使特征稀疏，L2为什么能解决(或者减轻) 过拟合
	   (L1范数，使权重为0，对应的特征则不起作用，使特征稀疏稀疏矩阵) 
	(3) Lasso、线性回归、逻辑回归、l1 l2 正则有什么影响，
	(4) 理解：L1正则先验分布是Laplace分布，L2正则先验分布是Gaussian分布
	(5) 口述一下l1参数分布的推导(牛皮) , l1在0处不可导，怎么处理:利用坐标轴下降法或者proximal operator：http://roachsinai.github.io/2016/08/03/1Proximal_Method/
	
	
7、--各种激活函数的优缺点
	(1) 激活函数有什么用？各种激活函数（sigmoid，tanh，ReLU, leaky ReLU, PReLU, ELU （ReLU变体））介绍一下，优缺点及适用场景
	(3) 深度学习用relu激励函数，为什么，好处是什么
	(2) 比如sigmod的问题在哪里，relu是怎么解决的，relu的问题在哪里，有没有对应的解决算法(prelu) ，介绍 Leaky Relu 并写公式 leakyrelu解决了梯度消失问题吗
	(4) relu激励函数训练数据时会让神经元失活，训练之后进行应用时有什么注意的吗，训练时失活的神经元在应用时怎么办

	
	
8、--Softmax 相关
    (1) Softmax的原理是？反向传播、梯度公式推导，代码实现
	(2) 为什么softmax是指数形式
	(3) LR + softmax做多分类区别
	(4) softmax、多个logistic的各自的优势？
	(5) 介绍分层 softmax，还有负采样？怎么负采样？
   （6）softmax 怎么防止溢出
	
	
9、-- 损失函数(Loss) 
	(1) 常用的 Loss 函数 (MSE /huber loss/cross entropy/指数损失 )  
	(2) cross entropy的原理是什么？反向传播机制是什么？交叉熵的公式伪代码
	(3) 为什么分类用交叉熵而不用MSE(均方误差) ？
	(4) 多标签分类怎么解决，从损失函数角度考虑	
	(5) Loss不降、不收敛的原因和解决方法
	(6) 损失函数正则项的本质是什么?
	(7) 样本不均衡怎么搞(重点考核损失函数优化 focal loss) 
   
	  
10、-- 批归一化Batch Normalization原理
	(1) BN 层的原理，相关公式，优化过程，优化的是什么，为什么BN有泛化能力的改善. 
	(2) BN有哪些需要学习的参数
	(3) BN 训练，测试区别
	(4) BN如何在inference是加速
	(5) BN在inference的时候用到的mean和var怎么来的：类似于滑动平均
        BN跨卡训练怎么保证相同的mean和var， 问了面试官是SYNC
	(6) BN、LN、IN、GN原理及适用场景,共性和特性
	   -- BN和GN的区别？各有什么优缺点？
	(7) 如果数据不是高斯分布，bn后怎么恢复


11、- Dropout
	(1) Dropout 的原理。为什么能防止过拟合？代码实现
	(2) dropout在训练和测试时不同，怎么保证测试结果稳定
	(3) dropout的随机因子会对结果的损失有影响吗
	(4) Dropout是失活神经元还是失活连接


12、-- 梯度消失和梯度爆炸：
	(1) 梯度消失与梯度爆炸原因？解决方案
	(2) 从数学层面分析一下
	(3) 梯度下降现在为什么用的少

	(4) 解决梯度消失一般都用什么损失函数
	(5) ResNet如何解决梯度消失？公式写一下
  
13、- 过拟合、欠拟合(overfitting,underfitting) 的原因及解决方法,解释为什么有效

14、--
	(1) 如何解决机器学习中的数据不平衡问题
	(2) 如何解决深度学习中的数据不平衡问题(重点考核损失函数优化，正负样本不均衡时的解决方案--Focal-Loss 、GHM)  
	(3) 如果把不平衡的训练集(正负样本1：3) 通过降采样平衡后，那么对于平衡后的AUC值和预测概率值有怎样的变化；
	(4) 如何进行数据预处理，如何进行数据归一化
	(5) 为什么要归一化？(消除数据量纲差，可以剔除一些异常值，会使得模型收敛快一些也好一些，计算友好度也会稍微好一些) 
	(6) 数据增强

15、- 深度学习常用的optimizer(优化器) 
	(1) 介绍一下你经常用的optimize
	(2) 凸优化了解吗？牛顿法、SGD、最小二乘法，各自的优势。
	(3) loss优化的几个方法(sgd、动量、adam) 
	(4) Adam优化器的迭代公式
	(5) 动量法的表达式，随机梯度下降相比全局梯度下降好处是什么
	(6) SGD每步做什么，为什么能online learning
	(7) 你一般用哪个优化器，为什么用它？
    (9) 学习率调节策略	 
	 
16、- 深度学习训练
	(1)深度学习中的batch的大小对学习效果有何影响？
	(2)深度学习bachsize怎么训练，不考虑硬件约束，batch是否越大越好



目标检测

	-- 目标检测中，如果检测到的目标较小时，框向内缩，该怎么解决
    -- 目标检测中如何解决目标尺度大小不一的情况 (图像金字塔，特征金字塔，Inception block)  https://www.cnblogs.com/E-Dreamer-Blogs/p/11442927.html

Faster-rcnn 相关
    (1) RCNN系列模型的区别， Faster R-CNN网络做了哪些改进/优化
   	(2) 项目中使用Faster rcnn，请问Faster rcnn的优势是什么，为什么在这个项目使用Faster rcnn
   	(3) Faster-rcnn RPN的作用和原理，RPN怎么计算 box 的实际坐标
   	(4) ROI pooling 的主要作用是什么（图片不同尺寸输入）？RoI pooling和RoI align区别
   	(5) Faster rcnn anchor机制，分别说一下 RPN阶段两种Loss分别是什么？
   	(6) faster-rcnn 损失函数，优化函数
   	(7) Faster-rcnn有什么不足的地方吗？如何改进？faster-rcnn怎么优化
    (8) 为什么回归损失中用smooth L1 (faster-rcnn) 
    

  -- YOLOV1~V3系列介绍，以及每一版的改进，优缺点介绍。yolov3中的anchor怎么生成的, 写出 YOLOv3 的损失函数
  -- YOLO中如何通过 K-Means 得到 anchor boxes？
  -- yolov2中聚类是怎么做的
  -- Anchor大小、长宽比选取？我说了业界常用的方法(YOLO9000中的方法) ，并提了一个更优的方法
  -- 如果YOLOV3采用Focal loss会怎么样？
  -- YOLOv3在小缺陷检测上也很好，RPN上和two-stage的有什么区别
  -- SSD、yolo、Fast RCNN 的区别。
  -- yolo跟ssd的损失函数是什么样，有啥缺点，yolo和ssd中正样本怎么确定的 
  -- FPN结构 
  -- Faster RCNN和SSD有啥不同，为啥SSD快？(不做Region Proposal，one-stage的) 
  
  
  
 
  -- ResNet相关。描述、介绍特点、介绍为什么效果不会随着深度变差
     ResNet V2 主要研究了什么问题, resnet缺点
  -- 残差网络为什么能解决梯度消失的问题
  -- resnet两种结构具体怎么实现，bottleneck的作用，为什么可以降低计算量，resnet参数量和模型大小
  -- LSTM为什么能解决梯度消失/爆炸的问题
  -- skip connection有什么好处？---- 推了下反向传播公式，根据链式法则，梯度可以直接作用于浅层网络。
  -- 为什么 DenseNet 比 ResNet 更耗显存？
  -- 相同层数，densenet和resnet哪个好，为什么？
  -- ResNet解决了什么问题
  -- 训练深层的神经网络，会遇到梯度消失和梯度爆炸（vanishing/exploding gradients）的问题，影响了网络的收敛，但是这很大程度已经被标准初始化（normalized   	   -- initialization）和BN（Batch Normalization）所处理。 当深层网络能够开始收敛，会引起网络退化（degradation problem）问题，即随着网络深度增加，准确率会饱和，甚至下降。这种退化不是由过拟合引起的，因为在适当的深度模型中增加更多的层反而会导致更高的训练误差。    ResNet就通过引入深度残差连接来解决网络退化的问题，从而解决深度CNN模型难训练的问题。

  
  
  -- RetinaNet 介绍， Focal loss， OHEM(online Hard example mining) 到底比focal loss差再哪里了
	 如何解决前景背景数量不均衡
     Focal Loss是如何进行难分样本挖掘的
	 Focal Loss 与 GHM

  -- VGG，GoogleNet，ResNet等网络之间的区别是什么？
  -- Inception(V1-V4) 网络结构以及优缺点
 
 轻量化模型
  -- MobileNet v1 v2 介绍和区别，MobileNetV2中1x1卷积作用
  -- MobileNet V2中的Residual结构最先是哪个网络提出来的，MobileNetV2 module的参数量和FLOPs计算
  -- shufflenet 算法题：random_shuffle的实现
  --深度可分离卷积 原理，为什么降低计算量，口述计算，减少了多少

   
   
   
  ---- Anchor-free 目标检测-----

  -- 介绍常见的 Anchor free 目标检测算法
  -- 介绍Anchor based 和Anchor free目标检测网络的优缺点
  -- 介绍 anchor-based和anchor-free两者的优缺点
  -- CornerNet介绍，CornerPooling是怎么做的，怎么解决cornernet检测物体合并为一个框的问题  
  -- CenterNet具体是如何工作的，介绍一下其损失函数
  
 

  -- 介绍目标检测中的多尺度训练/测试？
  -- 一个类似多标签训练的问题，他的loss是怎么算
  -- 在训练网络过程中怎么共享反向梯
  
 
扩展问题

1、 分类网络训练样本有噪声(错误标注) 怎么办
2、 分类网络样本不均衡怎么办
3、 分类网络想要分很细的类(比如阿拉斯加和哈士奇) 怎么办 细粒度分类
- 多标签分类怎么解决，从损失函数角度考虑
 
开放性问题，怎么处理特征、怎么选择模型
  
分割：
  -- deeplab v3如何改进，训练过程
  -- 说一下deeplab。它与其他state of art的模型对比
  -- deeplab的亮点是什么， 你认为deeplab还可以做哪些改进？
  -- 介绍deeplabv3,画出backbone
  -- 介绍金字塔池化，ASPP，深度可分，带孔卷积， PSPNet中PSP
  -- 语义分割中CRF的作用,介绍一下 CRF的原理
  -- HMM 和 CRF的区别
  -- CRF 怎么训练的(传统+深度学习) 
  -- 为什么深度学习中的图像分割要先编码再解码？
  -- BN在图像分割里面一般用吗？
  -- mask rcnn如何提高mask的分辨率，
  -- deeplabv3的损失函数
  -- 剪枝压缩，些精简网络 (tplink) 
  -- 介绍Mimic知识蒸馏是怎么做的
  -- 语义分割评价指标 Miou
  -- 串联与并联的ASPP都需画出。论文中认为这两种方式哪种更好？
      我答了并联更好，串联会产生Griding Efect。
      问：如何避免Griding Efect--网格效应
  -- 代码：mIOU(图像分割的通用评估指标) 的代码实现，使用numpy(我直接用了python) 




----自注意力机制， Attention-----

  -- Attention对比RNN和CNN，分别有哪点你觉得的优势
  -- 写出Attention的公式
  -- Attention机制，里面的q,k,v分别代表什么
  -- 为什么self-attention可以替代seq2seq


  -- 出现漏检、误检，怎么解决？
  -- anchor-free和anchor-based的优缺点



--- code 编程
 -- 卷积底层的实现方式(如caffe里面的img2col) 
 -- 手撕 IoU,NMS,SoftNMS代码
 -- 解释mAP，具体怎么计算？
 -- nms很耗时吗？ 时间复杂度？ 一般预测时会有多少个候选框？
 -- numpy实现交叉熵
 -- 例如计算flops，卷积维度变换的公式推导，卷积是如何编程实现的；
 -- pytorch中多卡训练的过程是怎样的？说下gather scatter是怎么做的？
 -- 多卡训练的时候batchsize变大了精度反而掉了，这是为什么？有想过怎么解决吗？
 -- 每张卡都有模型的话BN的参数一样吗？
 -- 设计一个在CNN卷积核上做dropout的方式
 -- PyTorch的高效convolution实现
 -- Python中不可变和可变数据类型
 -- PyTorch 不用库函数如何实现多机多卡
 -- dataloader 简单写



-- 场景分析--
--训练集loss上升，验证集loss保持基本不变，为什么
-- one-stage和two-stage在小目标上的区别
- 小目标检测有哪些trick
- 小目标不好检测，有试过其他的方法吗？比如裁剪图像进行重叠
 如何解决小目标： 为什么要深层、浅层featureMap concat？提了点细节和我踩的坑，需要数量级上的调整，不然深层的feature可能会被压制。
 Cascade的思想? 
- 单阶段比双阶段难训练？
- 数据清洗方面有什么优化的想法
- 模型加速的方法：答：network pruning和dw conv --shufflenet和mobilenet
- Focal Loss 为什么目标检测中提出，对分类真的有用吗，这个类似于开放性问题，交流了一下对于focalloss的理解，因为目标检测中一是类别不均衡,二是负例易分，所以focal loss有用，但是对于分类值得商讨。

监督学习和非监督学习举例

---实际操作相关

关于神经网络的调参顺序?
 如果训练集不平衡，测试集平衡，直接训练和过采样欠采样处理，哪个更好
 介绍一下调参的经验，这个需要自己总结
 如何训练模型、调优
     根据需求(前向传播时间、模型大小) ，确定模型和基础网络，跑第一版模型。(举了个栗子) 
     判断模型是否出现过拟合的情况，来决定下一步的优化方向。
     结果分析(confusionMatrix等) ，分析问题，将论文中的方法套上去，如果没有自己创造。(又举了个栗子) 

 - 256*256*3 -> 128*128*64的卷积，stride，padding和待优化的参数有多少
 如果网络初始化为0的话有什么问题
 讲一下调参的经验，lasso有什么参数要调，你在观测到什么情况会对lasso的参数做什么调整
 讲一下论文的模型，输出是什么，交叉熵表示什么含义，用了什么优化算法，adam和SGD的关系、batch size和epoch的平衡


 零样本分类问题。如果测试时出现一个图片是训练时没有的类别，怎么做
 


 
- 为什么mobileNet在理论上速度很快，工程上并没有特别大的提升？先说了卷积源码上的实现，两个超大矩阵相乘，可能是group操作，是一些零散的卷积操作，速度会慢。说应该从内存上去考虑。申请空间？

为什么用multi Loss？多loss权重如何选？训练普通的模型使其收敛，打印反向传播梯度的大小，这表示该task的难度，以此作为loss的权重，然后我补充说了下可以搞一个动态的loss权重，根据一段时间窗口来决定loss的权重。
就项目中数据处理方式做了详细的询问，生成的多张数据集如何使用，缺失值的处理需要考察到哪些问题，均值填充是否科学等



训练加速有什么办法？(答加大batch size，或者先adam再SGD) 

如果加大batch size需要调节哪些超参数(答加大learning rate，增加epoch) 

基本的深度学习知识，CNN，RNN，SGD，BN，各种训练Trick，要熟悉
时间片t的计算依赖t-1时刻的计算结果，这样限制了模型的并行能力；
顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM依旧无能为力。
LSTM比rnn的好处，乘变加，梯度信息有
如何做pytorch的部署与热更新
缺失值如何处理，什么情况下均值、众数，什么情况下丢弃特征。

- 编程题：用 PyTorch写一下大致的train val的流程


--算法题:
-- n*n的feature map上执行m*m的最大池化，步长1，padding m/2，设计算法并求时间复杂度
-- 直接暴力解，复杂度O(m*m*n*n) 
-- 二维的滑动窗口最大值问题，时间复杂度O(n)  n行向量，每个向量求滑动窗口大小为m的滑动窗口最大值，行之间无相关性，可并行。再计算列，同理O(n) , 整体时间复杂度O(n) 。


如何将高维的变量映射到低维？

总结：商汤的面试，非常注重基础，喜欢能说出自己想法的候选人，博主在面试中多次和面试官argue，比如在答第10题时，博主犹豫了一段时间，面试管提示：你听说过PCA降维吗， 这个问题可以通过PCA解决。我解释说，PCA并不能完全解决您的这个问题，因为PCA只能解除线性相关，无法解决高阶相关性，可以考虑Kernel PCA



